#+TITLE: Medical Risk Prediction Models
#+Author: \vspace{3em}\newline In memory of my very good friend Michael W Kattan
#+Latex_header:\institute{}
#+DATE: 
#+LaTeX_HEADER: \subtitle{With Ties To Machine Learning}
#+setupfile:~/emacs-genome/snps/org-templates/setup-beamer.org
* Intro

*** Outline [fn:1]

- Why should I care about statistical prediction models?
- I am going to make a prediction model. What do I need to know?
- How should I prepare for modeling?
- I am ready to build a prediction model
- Does my model predict accurately?
- How do I decide between rival models?
- What would make me an expert?
- Can't the computer just take care of all of this?

[fn:1] Medical risk prediction models: with ties to machine learning. Chapman and Hall/CRC, 2021.

*** Right on 

/The only useful function of a statistician is to make predictions, and thus to provide a basis for action/ -- W. Edwards
Deming \vfill

There are many ways to make a model, and every modeling expert has
preferences regarding the general approach and tuning.
\vfill


*** 

  [[~/metropolis/Teaching/causal-prediction-workshop/lecturenotes/figure-2-predictionmodeltimeline.pdf]]


*** 

  [[~/metropolis/Teaching/causal-prediction-workshop/lecturenotes/figure-2-blackbox-strategy.pdf]]

***   
  [[~/metropolis/Teaching/causal-prediction-workshop/lecturenotes/figure-2-blackbox-model.pdf]]

*** Notation

- Outcome :: $$Y(t)= \begin{cases}
      0 & \text{event-free or competing risk}\\
      1 & \text{event of interest before time t}
    \end{cases}$$
- Predictors :: \qquad\(X = (X^{1},\dots,X^{p})\)
- Dataset :: \qquad \(D_n=(Y_1(t),X_1, Y_2(t),X_2, \dots, Y_n(t),X_n)\)

#+begin_export latex
\begin{align*}
\text{Building the model}&\qquad\qquad\ensuremath{r}: \ensuremath{D}_n\mapsto \ensuremath{r}(\ensuremath{D}_n)=\hat M_n\\[2em]
\text{Using the model}&\qquad\qquad \hat M_n : X_{new}\mapsto \hat M_n(X_{new})\in [0,1]\\[2em]
\text{Example: logistic regression} & \qquad\qquad \hat M_n(X) = \operatorname{expit}(\hat\alpha_n + \hat\beta_n X)
\end{align*}
#+end_export

*** Measuring prediction performance

**** Calibration:
\begin{equation*}
p\mapsto P\{Y_i(t)=1| \hat M_n(X_i)=p\}
\end{equation*}

**** Discrimination:

\begin{equation*}
  \operatorname{AUC}(t) = \operatorname{P}(\hat M_n(X_i)\ge \hat M_n(X_j)|Y_i(t)=1,Y_j(t)=0)
\end{equation*}

**** Overall accuracy:
\begin{equation*}
  \operatorname{Brier\ score}(t) = \operatorname{E}\left\{Y_i(t)-\hat M_n(X_i)\right\}^2
\end{equation*}

*** 

Interpretation of a prediction performance measure should always
involve a benchmark, ideally that set by a rival prediction model.
\vfill

#+name: fig:1
#+ATTR_LATEX: :width 0.7\textwidth
  [[~/metropolis/Teaching/causal-prediction-workshop/lecturenotes/fig-5-auroc-thresholds.png]]

In a homogeneous population, even the perfect model can have low
discrimination ability (AUC/AUCROC). In a heterogeneous population, even a
bad model can have a high AUC.

*** Rare biomarkers ... 
#+ATTR_LATEX: :width 1.0\textwidth
  [[~/metropolis/Teaching/causal-prediction-workshop/lecturenotes/fig-2-newmarker.pdf]]

... do not show change overall predictive performance 

*** 


Benchmark values for the AUC (concordance index) and Brier score at any fixed prediction time horizon t.

| Benchmark prediction             | AUC | Brier score | Interpretation     |
|----------------------------------+-----+-------------+--------------------|
| 50% always                       | 50% |         25% | useless or harmful |
| Overall event probability always | 50% |  see Figure | useless            |
| Coin toss                        | 50% |         50% | harmful            |
| Uniform [0,1]                    | 50% |         33% | harmful            |

Being a rank statistic, the AUC is blind to
miscalibration of the predicted risks.
Hence, it cannot stand alone to assess models with respect to predictive accuracy.

*** Benchmark: the null model ignores the predictor variables 

#+ATTR_LATEX: :width 0.7\textwidth
[[file:figure-5-theoretical-brier-score.pdf]]

A good model outperforms the null model.

 
*** Example
:PROPERTIES:
:BEAMER_opt: shrink=25
:END:

#+ATTR_LATEX: :options otherkeywords={}, deletekeywords={}
#+BEGIN_SRC R  :results output example  :exports both  :session *R* :cache yes  
library(riskRegression)
train <- readRDS("./practicals/data/type1-diabetes-train.rds")
test <- readRDS("./practicals/data/type1-diabetes-test.rds")
head(train)
#+END_SRC
\pause
#+RESULTS[(2025-08-25 09:47:25) 0165becd143d3da2b296016b94252399682b4e20]:
#+begin_example
             pid      age sex_male diabetes_duration smoking motion   steno_prs HBA1C_pre_trt urine_albumin_pre_trt LDL_pre_trt SBP_pre_trt
1 train-28abed0e 52.92496        0          40.74757       0      0 -0.13899942      39.53003             59.187676   4.1392304    115.9685
2 train-f07f8cea 42.70053        0          36.11731       0      1  0.40351668      68.36319             42.915588   2.9280214    110.5099
3 train-f343cabb 47.57017        0          29.81477       0      0  0.54502533      52.47459             73.934960   3.7859522    110.9832
4 train-beb0e521 50.20713        0          40.73831       0      0  0.23982291      58.68347             46.645150   3.2022863    114.1410
5 train-05d45f09 40.76193        0          22.58828       0      1 -0.55949839      39.83002              7.486091   1.2928645    112.3791
6 train-82772fab 45.96690        0          28.94583       0      1  0.04637945      66.12423             96.794598   0.9119395    109.5785
  eGFR_pre_trt statin HBA1C_post_trt urine_albumin_post_trt LDL_post_trt SBP_post_trt eGFR_post_trt cvd_5year
1    104.03674      0       49.51156              95.958599     8.816790    116.41349     124.43999         0
2     90.13102      0       35.53323              44.175849     6.772076     97.39556     100.62088         0
3    102.67096      0       75.10901              96.535995     6.853455    135.02247      69.04463         0
4    113.77416      0       72.04421              39.454438     4.687593    130.34618      78.67379         0
5    104.25661      0       28.79908               5.671578     1.392458    105.75112     165.02973         0
6     98.40765      0       68.75695             230.856313     1.389660    124.03086      69.00498         0
#+end_example

*** Conventional model and experimental model

#+ATTR_LATEX: :options otherkeywords={}, deletekeywords={model}
#+BEGIN_SRC R  :results output :exports code  :session *R* :cache yes  :eval (never-plain-export)
# Logistic regression similar to Steno 1 risk engine formula
conventional_model <- glm(cvd_5year~sex_male + age + diabetes_duration + smoking + motion + HBA1C_post_trt + urine_albumin_post_trt + LDL_post_trt + SBP_post_trt + eGFR_post_trt,
             data = train, family = "binomial")
# Logistic regression with interactions and reduced number of variables
experimental_model <- glm(cvd_5year~sex_male *SBP_post_trt + age + I(age>40) * eGFR_post_trt + diabetes_duration + smoking + motion,
             data = train, family = "binomial")
#+END_SRC

*** Predicted risks for a single subject

#+ATTR_LATEX: :options otherkeywords={}, deletekeywords={model}
#+BEGIN_SRC R  :results output :exports both  :session *R* :cache yes  :eval (never-plain-export)
data.frame("subject id" = c(24,24),
           "model" = c("conventional","experimental"),
           "risk prediction" = c(predictRisk(conventional_model,newdata = test[24,]),
                                 predictRisk(experimental_model,newdata = test[24,])))
#+END_SRC

#+RESULTS[(2025-08-25 10:14:16) 9ee2a50cf33847f8ed467a4809739426159a4b19]:
:   subject.id        model risk.prediction
: 1         24 conventional       0.1826537
: 2         24 experimental       0.4429426

*** Evaluating model performance 

#+ATTR_LATEX: :options otherkeywords={}, deletekeywords={model,c}
#+BEGIN_SRC R  :results output example  :exports both  :session *R* :cache yes  
x <- Score(list("Conventional model" = conventional_model,"Experimental model" = experimental_model),
           data = test,
           formula = cvd_5year~1,
           summary = "risks",
           plots = c("roc","cal"))
summary(x,what = "score")
#+END_SRC

#+RESULTS[(2025-08-25 10:18:18) 9fa46e4cd2092fdb8b56bf99e7837487d53b1701]:
: $score
: Key: <Model>
:                 Model          AUC (%)     Brier (%)
:                <fctr>           <char>        <char>
: 1:         Null model             <NA> 6.1 [4.9;7.4]
: 2: Conventional model 88.3 [84.6;92.0] 4.6 [3.7;5.5]
: 3: Experimental model 87.8 [84.1;91.5] 4.9 [4.0;5.8]


*** Predicted risks

#+BEGIN_SRC R :results file graphics :file ~/metropolis/Teaching/causal-prediction-workshop/lecturenotes/day1-lecture2-example-predicted-risks.pdf :exports code :session *R* :cache yes
plotRisk(x)
#+END_SRC

#+RESULTS[(2025-08-25 10:22:14) 82796f471849557eaae702de41adfc59dc2348ec]:
[[file:~/metropolis/Teaching/causal-prediction-workshop/lecturenotes/day1-lecture2-example-predicted-risks.pdf]]

#+ATTR_LATEX: :width 0.7\textwidth
[[file:./day1-lecture2-example-predicted-risks.pdf]]

*** Discrimination

#+BEGIN_SRC R :results file graphics :file ~/metropolis/Teaching/causal-prediction-workshop/lecturenotes/day1-lecture2-example-roc.pdf :exports code :session *R* :cache yes
plotROC(x)
#+END_SRC

#+RESULTS[(2025-08-25 10:23:07) 1f1d6addba608f8b7fc42811d546a2750340efc9]:
[[file:~/metropolis/Teaching/causal-prediction-workshop/lecturenotes/day1-lecture2-example-roc.pdf]]

#+ATTR_LATEX: :width 0.7\textwidth
[[file:./day1-lecture2-example-roc.pdf]]

*** Calibration

#+BEGIN_SRC R :results file graphics :file ~/metropolis/Teaching/causal-prediction-workshop/lecturenotes/day1-lecture2-example-cal.pdf :exports code :session *R* :cache yes
plotCalibration(x)
#+END_SRC

#+RESULTS[(2025-08-25 10:23:49) 02d782a1b53b3012e6d09c323438e7c5b7700d8d]:
[[file:~/metropolis/Teaching/causal-prediction-workshop/lecturenotes/day1-lecture2-example-cal.pdf]]

#+ATTR_LATEX: :width 0.7\textwidth
[[file:./day1-lecture2-example-cal.pdf]]


*** Conditional versus expected performance

Conditional prediction performance is the performance of a risk
prediction model conditional on a single-purpose dataset. 
\vfill

A researcher who provides a risk prediction model for clinical
application is naturally interested in the conditional performance of
the model.
\vfill

The conditional prediction performance can be assessed by
an external validation dataset or, with some limitations, using data
splitting.  

*** Conditional versus expected performance

Expected prediction performance is the performance
of a modeling algorithm.
It is the average performance across all the
prediction models that a modeling algorithm can produce using all
possible learning datasets of a fixed sample size.
\vfill
A researcher who
has invented a new algorithm for building prediction models is
naturally interested in the average performance.

\vfill
The expected
prediction performance can either be assessed by computer simulation
of many datasets or by using cross-validation and bootstrap methods.

*** Uncertainty

A probabilistic prediction has built-in uncertainty
\vfill

Paradigm: A medical risk prediction model finds people in the data set
who were alike the current person, i.e., with similar values of the
risk predictors, and summarizes what happened to them.  \vfill

Any useful model will provide more reliable predictions for people who
are well represented in the data set than people at the border of
the data set.

*** Summary and outlook

A medical risk prediction model predicts the probability with which an
event occurs until a fixed prediction time horizon.
\vfill

Prediction performance (metrics) can be used to decide between rival models.

\vfill

The values of the prediction performance metrics do not have a direct
clinical interpretation!

\vfill How useful a model is depends on what it is used for \dots
